{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1762bb2f",
   "metadata": {},
   "source": [
    "# Clinical Health Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3037ab9",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28288223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/apathak2/.local/lib/python3.9/site-packages (4.40.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/apathak2/.local/lib/python3.9/site-packages (0.23.0)\n",
      "Requirement already satisfied: sentencepiece in /home/apathak2/.local/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: google-generativeai in /home/apathak2/.local/lib/python3.9/site-packages (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/apathak2/.local/lib/python3.9/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/apathak2/.local/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/apathak2/.local/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/apathak2/.local/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/apathak2/.local/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/apathak2/.local/lib/python3.9/site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/apathak2/.local/lib/python3.9/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.3 in /home/apathak2/.local/lib/python3.9/site-packages (from google-generativeai) (0.6.3)\n",
      "Requirement already satisfied: google-api-core in /home/apathak2/.local/lib/python3.9/site-packages (from google-generativeai) (2.18.0)\n",
      "Requirement already satisfied: google-api-python-client in /home/apathak2/.local/lib/python3.9/site-packages (from google-generativeai) (2.123.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/apathak2/.local/lib/python3.9/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: protobuf in /home/apathak2/.local/lib/python3.9/site-packages (from google-generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from google-generativeai) (1.8.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/apathak2/.local/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.3->google-generativeai) (1.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/apathak2/.local/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/apathak2/.local/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /home/apathak2/.local/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/apathak2/.local/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/apathak2/.local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.3->google-generativeai) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/apathak2/.local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.3->google-generativeai) (1.48.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers huggingface_hub sentencepiece protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342d4ee",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb7f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac6cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: NVIDIA A100-SXM4-80GB MIG 3g.40gb, n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Confirm that the GPU is detected\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74548886",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4f17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model_id = 'BioMistral/BioMistral-7B'\n",
    "model_path = 'models/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774c859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables to be used for saving and loading results\n",
    "code = \"diabetes\"\n",
    "text_column = \"Text\"\n",
    "label_column = \"Text_label\"\n",
    "prompt = \"prompt2\"\n",
    "data_path = \"data/test_diabetes_filtered.csv\"\n",
    "output = \"MIMIC_inference_small_\"\n",
    "chkpt=\"/checkpoint-2000\"\n",
    "save_name = output+\"_\"+code+\"_\"+prompt+'_yes_no_'+chkpt[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe43dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if code==\"oud\":\n",
    "    code_text = \"Opioid Use Disorder\"\n",
    "elif code == \"sud\":\n",
    "    code_text = \"Substance Use Disorder\"\n",
    "elif code == \"diabetes\":\n",
    "    code_text = \"Diabetes\"\n",
    "else:\n",
    "    print(\"Error in the code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb6717",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43667883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(examples):\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Given a patient's past medical history, predict whether the patient will have a future diagnosis of \" + code_text + \". Return 'Yes' or 'No' after the XML tag <Diagnosis>.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"### Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{INTRO_BLURB}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{examples[text_column]}\" if examples[text_column] else None\n",
    "\n",
    "    high_low_label = examples[label_column]\n",
    "    if high_low_label == \"High\":\n",
    "        t_label = \"Yes\"\n",
    "    elif high_low_label == \"Low\":\n",
    "        t_label = \"No\"\n",
    "    else:\n",
    "        print(\"There is some error with the label\")\n",
    "        \n",
    "    response_ground_truth = f\"{RESPONSE_KEY}\\n<Diagnosis>\"\n",
    "    \n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts_ground_truth = [part for part in [instruction, input_context, response_ground_truth] if part]\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt_ground_truth = \"\\n\\n\".join(parts_ground_truth)\n",
    "\n",
    "    # # Store the formatted prompt template in a new key \"text\"\n",
    "    # examples[\"prompt\"] = formatted_prompt\n",
    "\n",
    "    return formatted_prompt_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b738813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/apathak2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ffa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af811dba5e8542b5a1208cf88ca6b24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model =  AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5aebd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "700239e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7287e7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000117</td>\n",
       "      <td>A patient had 2 total visits to the hospital. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001217</td>\n",
       "      <td>A patient had 2 total visits to the hospital. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002428</td>\n",
       "      <td>A patient had 7 total visits to the hospital. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002769</td>\n",
       "      <td>A patient had 2 total visits to the hospital. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003299</td>\n",
       "      <td>A patient had 4 total visits to the hospital. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientId                                               Text  Label  \\\n",
       "0   10000117  A patient had 2 total visits to the hospital. ...    0.0   \n",
       "1   10001217  A patient had 2 total visits to the hospital. ...    0.0   \n",
       "2   10002428  A patient had 7 total visits to the hospital. ...    0.0   \n",
       "3   10002769  A patient had 2 total visits to the hospital. ...    0.0   \n",
       "4   10003299  A patient had 4 total visits to the hospital. ...    1.0   \n",
       "\n",
       "  Text_label  \n",
       "0        Low  \n",
       "1        Low  \n",
       "2        Low  \n",
       "3        Low  \n",
       "4       High  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b12fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_list = []\n",
    "for i, row in data.iterrows():\n",
    "    example = create_test_prompt(row)\n",
    "    examples_list.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb488dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8192\n",
    "batch_size = 1  # Adjust based on your GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token # or use tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "pred = []\n",
    "for i in range(0, len(examples_list), batch_size):\n",
    "    print(f\"Batch {i//batch_size+1}/{len(examples_list)//batch_size}\")\n",
    "    batch_examples = examples_list[i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        model_input = tokenizer(batch_examples, return_tensors=\"pt\", truncation=\"max_length\", max_length=max_length).to(device)\n",
    "\n",
    "        output_ = model(**model_input)\n",
    "        next_token_logits = output_.logits[0, -1, :]\n",
    "\n",
    "        # 2. step to convert the logits to probabilities\n",
    "        next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "\n",
    "        # 3. step to get the top 20\n",
    "        topk_next_tokens= torch.topk(next_token_probs, 20)\n",
    "\n",
    "        low_tokens = {\"No\", \"No\", \"N\", \"no\", \"NO\"}\n",
    "        high_tokens = {\"Yes\", \"Yes\", \"yes\", \"yes\", \"YES\", \"Y\"}\n",
    "\n",
    "        for j in range(len(batch_examples)):\n",
    "            top_k_probs = [(tokenizer.decode(idx), prob) for idx, prob in zip(topk_next_tokens.indices[j], topk_next_tokens.values[j])]\n",
    "            low_sum = 0\n",
    "            high_sum = 0\n",
    "\n",
    "            for k, v in top_k_probs:\n",
    "                if k in low_tokens:\n",
    "                    low_sum += v.item()\n",
    "                elif k in high_tokens:\n",
    "                    high_sum += v.item()\n",
    "            arr = [high_sum, low_sum]\n",
    "            low_high_probs = np.exp(arr) / np.sum(np.exp(arr), axis=0) # instead of softmax, normalize it\n",
    "            pred.append(low_high_probs[0])\n",
    "\n",
    "        # Clear cache and free memory\n",
    "        del model_inputs, outputs, next_token_logits, next_token_probs, topk_next_tokens, top_k_probs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {datetime.timedelta(seconds=elapsed_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63588eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results into a new df\n",
    "result_df = pd.DataFrame(columns =[\"PatientID\",\"Predictions\",\"Label\"])\n",
    "result_df[\"PatientID\"] = df[\"PatientId\"].values\n",
    "result_df[\"Predictions\"] = pred\n",
    "result_df[\"Label\"] = labels\n",
    "\n",
    "# Show results\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "result_df.to_csv(\"Predictions_\" +save_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19933c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show metrics\n",
    "roc = roc_auc_score(labels, pred)\n",
    "pr_auc = average_precision_score(labels, pred)\n",
    "\n",
    "print(\"ROC Area under curve: \", roc)\n",
    "print(\"Avg Precision Score: \", pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the uncertainity estimates\n",
    "## Change the output to probality score instead of yes/no\n",
    "## Change the output to probality score instead of high/low or something similar\n",
    "## Run the same model multiple times and note down the results and compare\n",
    "\n",
    "# Research what has been done and can be done further based on the uncertainity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f0192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbff271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
